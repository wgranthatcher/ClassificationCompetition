\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{array}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{url}
\usepackage{breakurl}
%\usepackage[breaklinks]{hyperref}
\usepackage{ragged2e}
\usepackage{subfig}
%\usepackage{subcaption}
%\usepackage{cleveref}
\usepackage[noabbrev]{cleveref}
%\usepackage{mwe}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage{fixltx2e}
\pagenumbering{arabic}
\usepackage{color}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage[labelsep=period]{caption}
%\captionsetup[figure]{margin={1cm,0cm}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\bbf}[1]{\mbox{\boldmath$#1$\unboldmath}}
\newcommand{\red}[1]{\textcolor{red}{\bf #1}}
\newcommand{\warn}[1]{}
%\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
%\renewcommand\thesubfigure{(\alph{subfigure})}
\def\linespaces{1.0}
\def\baselinestretch{\linespaces}

\usepackage{xcolor}
\usepackage[export]{adjustbox}
\usepackage{tabularx}


\begin{document}

\title{Classification Competition}

\newcommand{\superast}{\raisebox{9pt}{$\ast$}}
\newcommand{\superdagger}{\raisebox{9pt}{$\dagger$}}
\newcommand{\superddagger}{\raisebox{9pt}{$\ddagger$}}
\newcommand{\superS}{\raisebox{9pt}{$\S$}}
\newcommand{\superP}{\raisebox{9pt}{$\P$}}

\author{\IEEEauthorblockN{William G. Hatcher, Kevin McNamara}
\IEEEauthorblockA{Department of Computer and Information Sciences\\ Towson University, Maryland, USA 21252, USA \\Emails:  whatch2@students.towson.edu, kmcnamara@towson.edu}}

\maketitle

\begin{abstract}
Given labeled training and unlabeled testing datasets of forest cover types, we have conducted supervised learning to predict the class label "Cover\_Type". The data includes over 50 attributes of binary and quantitative numeric values. We have tested many different learning algorithm with a variety of settings, determining the Random Forest to perform the best. Our test accuracy, using a 70\%/30\% training/testing split of the \textit{training} dataset, was approximately 95\%. 
\end{abstract}

\begin{IEEEkeywords}
Data mining, Machine learning, Classification
\end{IEEEkeywords}

\section{Classification Methodology}\label{description}

We tested K-Nearest-Neighbors, Decision Tree, Random Forest, and Naive Bayes algorithms, which we denote as \textit{Basic Learning} and \textit{Improved Basic Learning} algorithms. Results can be seen in Tables~\ref{Table1} through \ref{Table4}. In Tables~\ref{Table1} and \ref{Table2}, we have K-Nearest-Neighbors initialized with 3 neighbors (KNN-3), Decision Trees with gini (DT-gini) and entropy (DT-entropy) as attribute splitting mechanisms, Random Forest with 10 nodes (RF-10), and Naive Bayes (NB-gaussian). In Tables~\ref{Table3} and \ref{Table4}, we have K-Nearest-Neighbors initialized with 7 neighbors (KNN-7), Decision Trees with entropy and minimum leeaves set to 5 (DT-entropy, min-leaf-5), Extra Tree with entropy (ET-entropy) as the attribute splitting mechanisms, Random Forest with 100 nodes (RF-100), and Multinomial Naive Bayes (NB-multinomial). 

In testing these Basic Learning mechanisms, because they trained quite quickly, we were able to train different models with Full Sets of the data (Full Set 1 and Full Set 2), with only the Binary attributes (Binary Only 1), such as 2702, 2703, 2704, etc., and with only Numeric attributes (Numeric Only 1 and Numeric Only 2). We then compared these different models to determine the impacts on our accuracy results. For example, from Table~\ref{Table1}, we see that the accuracy was the highest in the full sets of attributes. We also applied Min-Max normalization in our \textit{Improved Basic Learning} models for comparison, with only slight improvements. 

In addition to the basic learning models, we also analyzed \textit{Advanced Learning} and \textit{Improved Advanced Learning} models. These include Logistic Regression (LogReg), Neural Networks (NN-30x3), SVM, Bagging, and Boosting algorithms. The results of these models can be seen in Tables~\ref{Table5} and \ref{Table6}. Clearly, these models did not perform nearly as well as the basic algorithms, and also took significantly longer to train. 

Finally, based on the results noted above, we determined the Random Forest algorithm to perform the best. Thus we tested multiple versions of the RF algorithm with a variety of settings. In Table~\ref{Table7} we can see the results of this study. Here, the "-\#" indicates the number of estimators in the forest. So, for example, RF-250 has 250 estimators. In addition, we were also able to apply entropy as the decision criterion, denoted as "-entropy" in the table. Finally, the RF-250-entropy model in \red{red} performed the best, and was used to create our final \textit{Prediction.csv} file on the unlabeled test data. 

On an additional note, we applied TensorFlow and Keras machine learning libraries, as denoted in Table~\ref{Table8} to train dense neural networks of shapes 250-100-10-7 and 500-250-10-7. These were trained on a variety of epochs and batch sizes, reported as "\_epochs\_batch" in the table. Again, we trained the models on only binary data and only multivariate data ("-bin" and -"multi") to determine the performance of data subsets. These again performed worse. It is also notable that the final accuracies calculated, denote by Accuracy*, we not the same as the training accuracy reported by the TensorFlow API, and it is not clear what the difference is. For this reason, this method was not ultimately used.
 
% \begin{figure}[ht]
% 	\vspace{-3mm}
% 	\centering
% 	\includegraphics[width=0.3\textwidth]{./figures/"Elevation Density by Cover Type".png}\\
% 	\caption{EDIT: Bar charts, histograms, and KDE plots of some.}
% 	\label{fig1}
% \end{figure}
 
\begin{center}
	\begin{table}[ht]
		\centering \footnotesize
		\vspace{-2mm}
		\caption{Basic Learning - Accuracy}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set 1}&	\textbf{Full Set 2}&	\textbf{Binary Only 1}&	\textbf{Numeric Only 1}&	\textbf{Numeric Only 2} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{KNN-3}&	0.596&	0.596&	0.415&	0.596&	0.596\\ \hline
			\textbf{DT-gini}&	0.927&	0.927&	0.540&	0.897&	0.897\\ \hline
			\textbf{DT-entropy}&	\textbf{0.930}&	\textbf{0.930}&	0.540&	0.910&	0.909\\ \hline
			\textbf{RF-10}&	0.925&	0.927&	\textbf{0.546}&	\textbf{0.918}&	\textbf{0.917}\\ \hline
			\textbf{NB-gaussian}&	0.634&	0.634&	0.488&	0.634&	0.634\\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-2mm}
		\label{Table1}
	\end{table} \hfil
\end{center}

\begin{center}
	\begin{table}[ht]
		\centering \footnotesize
		\vspace{-2mm}
		\caption{Basic Learning - F1-Score}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set 1}&	\textbf{Full Set 2}&	\textbf{Binary Only 1}&	\textbf{Numeric Only 1}&	\textbf{Numeric Only 2} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{KNN-3}&	0.590&	0.590&	0.400&	0.590&	0.590\\ \hline
			\textbf{DT-gini}&	\textbf{0.930}&	\textbf{0.930}&	0.540&	0.900&	0.900\\ \hline
			\textbf{DT-entropy}&	\textbf{0.930}&	\textbf{0.930}&	0.540&	0.910&	0.910\\ \hline
			\textbf{RF-10}&	0.920&	\textbf{0.930}&	\textbf{0.550}&	\textbf{0.920}&	\textbf{0.920}\\ \hline
			\textbf{NB-gaussian}&	0.640&	0.640&	0.320&	0.640&	0.640\\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-2mm}
		\label{Table2}
	\end{table} \hfil
\end{center}

\begin{figure*}[!]
	\vspace{-4mm}
	\centering
	\includegraphics[width=0.19\textwidth]{./figures/"Soil Type/2702".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Soil Type/2703".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Soil Type/2704".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Soil Type/2705".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Soil Type/2706".png}\\
	%	\caption{Basic Block Example}
	
	\centering	
	\includegraphics[width=0.19\textwidth]{./figures/"Aspect".png}
	\includegraphics[width=0.19\textwidth]{./figures/"elevation".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Hillshade9am".png}
	\includegraphics[width=0.19\textwidth]{./figures/"HillshadeNoon".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Hillshade3pm".png}\\
	%	\caption{Basic Block Example}
	
	\centering
	\includegraphics[width=0.19\textwidth]{./figures/"HorizDistToFirePoints".png}
	\includegraphics[width=0.19\textwidth]{./figures/"HorizDistToHydrology".png}
	\includegraphics[width=0.19\textwidth]{./figures/"HorizDistToRoadways".png}
	\includegraphics[width=0.19\textwidth]{./figures/"VertDistToHydrology".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Slope".png}\\
	%	\caption{Basic Block Example}
	
	\centering
	\includegraphics[width=0.19\textwidth]{./figures/"Wilderness1".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Wilderness2".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Wilderness3".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Wilderness4".png}
	\includegraphics[width=0.19\textwidth]{./figures/"Cover_Type".png}\\
	%	\caption{Basic Block Example}
	
	%\captionsetup{justification=centering, width=0.9\textwidth}
	\caption{Bar charts, histograms, and KDE plots of some of the dataset attributes.}
	\label{fig2}
	\vspace{-4mm}
\end{figure*}

\begin{center}
	\begin{table}[b]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Improved Basic Learning - Accuracy}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set 1}&	\textbf{Full Set 2}&	\textbf{Full Set 1 min-max}&	\textbf{Full Set 2 min-max} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{KNN-7}&  0.596&	0.596&	0.875&	0.875 \\ \hline
			\textbf{DT-entropy, min-leaf-5}&  0.921&	0.921&	0.921&	0.921 \\ \hline
			\textbf{ET-entropy}&  0.825&	0.816&	0.841&	0.822 \\ \hline
			\textbf{RF-100}& \textbf{0.942}&	\textbf{0.942}&	\textbf{0.943}&	\textbf{0.943} \\ \hline
			\textbf{NB-multinomial}&  -	&-	&0.642&	0.642\\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table3}
	\end{table} \hfil
\end{center}

\begin{center}
	\begin{table}[b]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Improved Basic Learning - F1-Score}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set 1}&	\textbf{Full Set 2}&	\textbf{Full Set 1 min-max}&	\textbf{Full Set 2 min-max} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{KNN-7}&  0.590&	0.590&	0.870&	0.870 \\ \hline
			\textbf{DT-entropy, min-leaf-5}&  0.920	&0.920&	0.920&	0.920	 \\ \hline
			\textbf{ET-entropy}&  0.820&	0.820&	0.840&	0.820	\\ \hline
			\textbf{RF-100}& \textbf{0.940}&	\textbf{0.940}&	\textbf{0.94}0&	\textbf{0.940}	 \\ \hline
			\textbf{NB-multinomial}&  -&	-&	0.620&	0.620 \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table4}
	\end{table} \hfil
\end{center}


%\begin{figure*}[ht]
%	\vspace{-2mm}
%	\centering
%	\includegraphics[width=0.4\textwidth]{./figures/"Desc and Med Elevation".png}
%	\includegraphics[width=0.4\textwidth]{./figures/"Percentage of Tree Cover Types".png}\\
%	%	\caption{Basic Block Example}
%	
%	%\captionsetup{justification=centering, width=0.9\textwidth}
%	\caption{EDIT: Bar charts, histograms, and KDE plots of some of the dataset attributes.}
%	\label{fig3}
%\end{figure*}


\begin{center}
	\begin{table}[h]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Advanced Learning - Accuracy}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set}&	\textbf{Binary Only}&	\textbf{Numeric Only}&	\textbf{Numeric Only min-max} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{LogReg}&	0.717&	0.647&	\textbf{0.660}&	0.665\\ \hline
			\textbf{NN-30x3-sgd}&	\textbf{0.851}&	\textbf{0.652}&	0.487&	\textbf{0.804}\\ \hline
			\textbf{NN-30x3-lbfgs}&	0.810&	0.651&	0.481&	0.744\\ \hline
			\textbf{SVM-rbf}&	0.790&	0.651&	-&	0.770\\ \hline
			\textbf{Bagging}&	0.113&	0.126&	-&	0.655\\ \hline
			\textbf{Boosting}&	0.418&	0.613&	-&	0.613 \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table5}
	\end{table} \hfil
\end{center}

\begin{center}
	\begin{table}[h]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Advanced Learning - F1-Score}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{ACCURACY} &	\textbf{Full Set}&	\textbf{Binary Only}&	\textbf{Numeric Only}&	\textbf{Numeric Only min-max} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{LogReg}&	0.700&	0.630&	\textbf{0.630}&	0.640 \\ \hline
			\textbf{NN-30x3-sgd}&	\textbf{0.850}&	\textbf{0.640}&	0.240&	\textbf{0.800} \\ \hline
			\textbf{NN-30x3-lbfgs}&	0.810&	0.640&	0.370&	0.740 \\ \hline
			\textbf{SVM-rbf}&	0.810&	0.640&	-&	0.740 \\ \hline
			\textbf{Bagging}&	0.660&	0.740&	-&	0.640 \\ \hline
			\textbf{Boosting}&	0.580&	0.620&	-&	0.610 \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table6}
	\end{table} \hfil
\end{center}

\begin{center}
	\begin{table}[h]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Top Basic Learning - Random Forests}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{FULL DATA min-max} &	\textbf{Accuracy}&	\textbf{Precision}&	\textbf{Recall}&	\textbf{F1-Score} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{RF-250}&	0.943&	0.94&	0.94&	0.94\\ \hline
			\textbf{RF-500}&	0.943&	0.94&	0.94&	0.94\\ \hline
			\textbf{RF-1000}&	0.943&	0.94&	0.94&	0.94\\ \hline
			\textbf{RF-100-entropy}&	0.945&	\textbf{0.95}&	\textbf{0.9}5&	0.94\\ \hline
			\red{RF-250-entropy}&	\red{0.951}&	\red{0.95}&	\red{0.95}&	\red{0.95} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table7}
	\end{table} \hfil
\end{center}

\begin{center}
	\begin{table}[h]
		\centering \footnotesize
		\vspace{0.01cm}
		\caption{Top Advanced Learning - Dense Neural Networks with TensorFlow}
		\hspace{1cm}
		\begin{tabularx}{\linewidth}{ l  X  X  X  X }
			\hline
			\textbf{FULL DATA min-max} &	\textbf{Accuracy*}&	\textbf{Precision}&	\textbf{Recall}&	\textbf{F1-Score} \\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			\textbf{250-100-10-7\_1-64}&	0.937*	&0.78&	0.77&	0.76\\ \hline
			\textbf{250-100-10-7\_1-64-bin}&	0.899*&	0.62&	0.63&	0.61\\ \hline
			\textbf{250-100-10-7\_1-64-mult}&	0.917*	&0.69&	0.7&	0.69\\ \hline
			\textbf{250-100-10-7\_32-16}&	\textbf{0.968*}&	\textbf{0.90}&	\textbf{0.88}&	\textbf{0.89}\\ \hline
			\textbf{500-250-100-10-7\_32-16}&	0.958*&	0.84&	0.85&	0.84\\ \hline
			
			%&&&&&&&&&&&&\\[-2ex] \hline
			
			%\cellcolor{lightgray} & \cellcolor{lightgray} \\ \hline
		\end{tabularx}\newline
		\vspace{-0.05cm}
		\label{Table8}
	\end{table} \hfil
\end{center}


%\bibliographystyle{abbrv}
%\vspace{-0.01cm}
%\bibliography{ref}

\end{document}
